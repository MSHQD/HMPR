{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSHQD/HWR/blob/main/model_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmTitGiusn_o",
        "outputId": "62af0e52-ae04-4ebd-ae1a-d40fa8cafcc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTflWLKp-g6T",
        "outputId": "8f1a5686-7616-4959-ab1c-6854d807519c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hwb in /usr/local/lib/python3.11/dist-packages (0.0.15)\n",
            "Requirement already satisfied: bezier in /usr/local/lib/python3.11/dist-packages (from hwb) (2024.6.20)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from hwb) (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python>=4.1.1->hwb) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install hwb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqbOtzeCdpnn"
      },
      "source": [
        "# **Settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIEvhtTyqrMW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install -q datasets jiwer\n",
        "!pip install sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBUFmI0XIfJ9"
      },
      "outputs": [],
      "source": [
        "!ls -hl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gedlS8IL1YYm"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/microsoft/unilm\n",
        "!cp unilm/trocr/data_aug.py data_aug.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RRUTvxSenis"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch, torchvision\n",
        "import warnings\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljo9pZWxfbgj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN97lcHoxTqZ"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "def set_seed(seed: int = 42, set_torch=True):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    if set_torch:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApNMhPz30Jku"
      },
      "source": [
        "# **Global variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbzUqpMx0PR3"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/train_recognition_small\"\n",
        "TRAIN_DIR = \"/content/train_recognition_small/images\"\n",
        "# TRAIN_DIR = '/content/data'\n",
        "PROJECT_DIR = '/content/drive/MyDrive'\n",
        "MODEL_DIR = os.path.join(PROJECT_DIR, 'segmentation_models')\n",
        "TRAIN_BATCH_SIZE = 10\n",
        "model_type = 'small'\n",
        "VAL_BATCH_SIZE = 64\n",
        "img_size = (384, 384) #(256, 256) #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "148p8QO9yXFB"
      },
      "source": [
        "# **Load datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kr10vEVZf4zi"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip1 = '/content/drive/MyDrive/train_recognition_small.zip'\n",
        "extract_to1 = '/content/train_recognition_small'\n",
        "\n",
        "with zipfile.ZipFile(zip1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv9qfLg8yWen"
      },
      "outputs": [],
      "source": [
        "hack_data = pd.read_json('train_recognition_small/train_recognition_small/labels_small.json')\n",
        "hack_data = hack_data.rename(columns={\n",
        "    'file_name': 'image',\n",
        "    'text': 'label'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(hack_data.head())"
      ],
      "metadata": {
        "id": "xcznaIA0h8va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaIfY8Fc746n"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "hack_data['image'] = hack_data['image'].apply(lambda x: os.path.join('train_recognition_small/train_recognition_small/images', Path(x).name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WyFpz6PrF7V"
      },
      "source": [
        "## see datasets intersection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCQbaBzb_Zfm"
      },
      "outputs": [],
      "source": [
        "def dhash(image, hashSize=8):\n",
        "\tresized = cv2.resize(image, (hashSize + 1, hashSize))\n",
        "\tdiff = resized[:, 1:] > resized[:, :-1]\n",
        "\treturn sum([2 ** i for (i, v) in enumerate(diff.flatten()) if v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NE8TU_UE9zD3"
      },
      "outputs": [],
      "source": [
        "hack_hashes = [dhash(cv2.imread(path)) for path in tqdm(hack_data['image'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXkIplNcCURl"
      },
      "outputs": [],
      "source": [
        "# second_stage_hashes = [dhash(cv2.imread(path)) for path in tqdm(second_stage_data['image'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZlJf-UMCNjV"
      },
      "outputs": [],
      "source": [
        "# len(set(second_stage_hashes) | set(hack_hashes)), len(set(second_stage_hashes) & set(hack_hashes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4akcGali7gMD"
      },
      "outputs": [],
      "source": [
        "s = 'рядом'\n",
        "plt.imshow(plt.imread(hack_data[hack_data['label'] == s].iloc[0]['image']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0KvNdY5Dofz"
      },
      "outputs": [],
      "source": [
        "plt.hist([len(el.split()) for el in hack_data['label']])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgMVLm8lrnqn"
      },
      "source": [
        "# **Building VAL dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsgO7zLNL5F9"
      },
      "outputs": [],
      "source": [
        "# def get_one_words(df):\n",
        "#   mask = [len(el.split()) == 1 or (len(el.split()) == 2 and (len(el[0]) < 3 or len(el[1] < 3))) for el in df['label']]\n",
        "#   return df[np.array(mask)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ERNMfDGNshE"
      },
      "outputs": [],
      "source": [
        "hack_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRB2Y4uen0In"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_val = train_test_split(hack_data, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L30YiZslsb4X"
      },
      "outputs": [],
      "source": [
        "df_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUIX-EmasLTd"
      },
      "outputs": [],
      "source": [
        "df_val = df_val[['train_recognition_small' in el for el in df_val['image']]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeX1nNF7sd8m"
      },
      "outputs": [],
      "source": [
        "df_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0r84auRRSqD"
      },
      "outputs": [],
      "source": [
        "def random_show(df):\n",
        "  now = df.sample(1).iloc[0]\n",
        "  img = plt.imread(now['image'])\n",
        "  plt.imshow(img)\n",
        "  plt.title(now['label'])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGV3WnWPRkX5"
      },
      "outputs": [],
      "source": [
        "random_show(hack_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZNzHCE1iRcf"
      },
      "outputs": [],
      "source": [
        "df_train.shape, df_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SGKPi7x4o-Q"
      },
      "outputs": [],
      "source": [
        "plt.hist(hack_data['label'].apply(lambda x: len(x)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UxsVJXlsgl4"
      },
      "source": [
        "# **Define dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kmEnR22RBhw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, transforms, tokenizer, feature_extractor, max_target_length=64):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.max_target_length = max_target_length\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _load_file(self, path):\n",
        "      image = cv2.imread(path)\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      return image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        el = self.df.iloc[idx]\n",
        "        file_name = el['image']\n",
        "        text = el['label']\n",
        "\n",
        "        image = self._load_file(os.path.join(self.root_dir, file_name))\n",
        "        image = self.transforms(image=image)['image']\n",
        "\n",
        "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.tokenizer(text,\n",
        "                                padding=\"max_length\",\n",
        "                                max_length=self.max_target_length).input_ids\n",
        "\n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYym7KGFDAZm"
      },
      "outputs": [],
      "source": [
        "# from augmixations import HandWrittenBlot\n",
        "from hwb import HandWrittenBlot\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "class AlbuHandWrittenBlot(A.DualTransform):\n",
        "    def __init__(self, hwb, always_apply=False, p=0.5):\n",
        "        super(AlbuHandWrittenBlot, self).__init__(always_apply, p)\n",
        "        self.hwb = hwb\n",
        "\n",
        "    def apply(self, image, **params):\n",
        "        return self.hwb(image)\n",
        "\n",
        "\n",
        "class AlbuPadding(A.DualTransform):\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(AlbuPadding, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, image, **params):\n",
        "        zeros = np.zeros((128, 384, 3))\n",
        "        image = np.concatenate([zeros, image, zeros], axis=0)\n",
        "        return image.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0bzI6S2QfHs"
      },
      "outputs": [],
      "source": [
        "rectangle_info = {\n",
        "    'x': (None, None),\n",
        "    'y': (150, 220),\n",
        "    'h': (60, 100),\n",
        "    'w': (50, 80),\n",
        "}\n",
        "\n",
        "blot_params = {\n",
        "    'incline': (-10, 10),\n",
        "    'intensivity': (0.5, 0.9),\n",
        "    'transparency': (0.05, 0.4),\n",
        "    'count': (1, 3),\n",
        "}\n",
        "\n",
        "blots = HandWrittenBlot(rectangle_info, blot_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmeK1OBH6OwR"
      },
      "outputs": [],
      "source": [
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "data_transforms = {\n",
        "    'train': A.Compose([\n",
        "              A.Resize(128, 384),\n",
        "              AlbuPadding(always_apply=True),\n",
        "              AlbuHandWrittenBlot(blots, p=0.3),\n",
        "              A.Rotate(limit=[-7, 7]),\n",
        "              A.OneOf([\n",
        "                A.ToGray(always_apply=True),\n",
        "                A.CLAHE(always_apply=True, clip_limit=15),\n",
        "              ], 0.3)\n",
        "          ]),\n",
        "    'val': A.Compose([\n",
        "              A.Resize(128, 384),\n",
        "              AlbuPadding(always_apply=True),\n",
        "          ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krAhnVEIUVU1"
      },
      "outputs": [],
      "source": [
        "plt.imshow(data_transforms['train'](image=np.ones((140, 312, 3)) * 255)['image'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYL5t6uL6QqI"
      },
      "source": [
        "# **Define model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3YjE_Iq4QGli"
      },
      "outputs": [],
      "source": [
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Загружаем модель и процессор\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"raxtemur/trocr-base-ru\")\n",
        "processor = TrOCRProcessor.from_pretrained(\"raxtemur/trocr-base-ru\")\n",
        "# tokenizer = XLMRobertaTokenizer.from_pretrained('microsoft/trocr-small-handwritten')\n",
        "feature_extractor = processor.feature_extractor\n",
        "tokenizer = processor.tokenizer\n",
        "model.eval()\n",
        "\n",
        "# model = VisionEncoderDecoderModel.from_pretrained(f'microsoft/trocr-small-handwritten')\n",
        "# model = VisionEncoderDecoderModel.from_pretrained(f\"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_best_small_aug_nti2data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp7Pgz9063PS"
      },
      "outputs": [],
      "source": [
        "def set_requires_grad(model, value) :\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0it9jflpTdXd"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io9cOTJWNJRz"
      },
      "outputs": [],
      "source": [
        "model.decoder.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# set special tokens used for creating the decoder_input_ids from the labels\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "# make sure vocab size is set correctly\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "model.config.eos_token_id =  tokenizer.sep_token_id\n",
        "model.config.max_length = 64\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "# model.config.encoder.image_size = img_size\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zddTbpNrczNG"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(os.path.join(PROJECT_DIR, 'ocr_models', f'trocr-{model_type}-handwritten-tokenizer'))\n",
        "processor.save_pretrained(os.path.join(PROJECT_DIR, 'ocr_models', f'trocr-{model_type}-handwritten-feature-extractor'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OMTL1Nx6ZbE"
      },
      "source": [
        "# **Define loaders and try model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjsQlCcT2CEy"
      },
      "outputs": [],
      "source": [
        "from transformers import TrOCRProcessor\n",
        "\n",
        "train_dataset = IAMDataset(root_dir='./',\n",
        "                           df=df_train,\n",
        "                           transforms=data_transforms['train'],\n",
        "                           tokenizer=tokenizer,\n",
        "                           feature_extractor=processor)\n",
        "\n",
        "val_dataset = IAMDataset(root_dir='./',\n",
        "                         df=df_val,\n",
        "                         transforms=data_transforms['val'],\n",
        "                         tokenizer=tokenizer,\n",
        "                         feature_extractor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b2Ku66E2ftR"
      },
      "outputs": [],
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBZsWBkxY47r"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16,)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=16,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFItnut1S9MX"
      },
      "outputs": [],
      "source": [
        "val_batch = next(iter(val_loader))\n",
        "val_batch['pixel_values'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DyIkrMuKqe_"
      },
      "outputs": [],
      "source": [
        "for k, v in val_batch.items():\n",
        "  val_batch[k] = v.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eSGCscOLxnr"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "y = model(**val_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IferYD8G9V6X"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "cer_metric = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTSRGN-K562T"
      },
      "outputs": [],
      "source": [
        "def compute_cer(pred_ids, label_ids):\n",
        "    pred_ids = pred_ids.cpu().numpy()\n",
        "    label_ids = label_ids.cpu().numpy()\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return cer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkI_tMCiOjOe"
      },
      "source": [
        "# **Train model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noZUWl7PAu0V"
      },
      "outputs": [],
      "source": [
        "def plot_images(images_for_show):\n",
        "  \"\"\"Строит изображение на одном графике\"\"\"\n",
        "  fig = plt.figure(figsize=(16, 16))\n",
        "\n",
        "  columns = len(images_for_show)\n",
        "  rows = 1\n",
        "  for i in range(1, columns*rows +1):\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.clip(images_for_show[i - 1], 0, 1))\n",
        "\n",
        "  fig.subplots_adjust(wspace=0.1, hspace=0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GRGyau0Q1Lj"
      },
      "outputs": [],
      "source": [
        "def show_random_predict(model, test_loader, batch=None):\n",
        "  \"\"\"Выводит необходимую информацию после каждой эпохи\"\"\"\n",
        "  if batch is None:\n",
        "    batch = next(iter(test_loader))\n",
        "\n",
        "\n",
        "  outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
        "  now = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
        "  plot_images(torch.moveaxis(batch['pixel_values'][:4], 1, -1).detach().cpu().numpy())\n",
        "  print([tokenizer.decode(pred.cpu().numpy(), skip_special_tokens=True) for pred in outputs[:4]])\n",
        "  print([tokenizer.decode(el.cpu().numpy(), skip_special_tokens=True) for el in batch['labels'][:4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQNc6R_Rb70P"
      },
      "outputs": [],
      "source": [
        "show_random_predict(model.train(True), val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcwBQ4cGVku-"
      },
      "outputs": [],
      "source": [
        "show_random_predict(model.train(False), val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXA9SeCyD79l"
      },
      "outputs": [],
      "source": [
        "val_batch = next(iter(val_loader))\n",
        "train_batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veobh5dEOmoy"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train_epoch(model, batch_gen, criterion, optimizer, is_train = True, full_cer = False) :\n",
        "    epoch_loss = 0.0\n",
        "    count = 0\n",
        "    cer = 0.0\n",
        "    cnt_batches = 0\n",
        "    # model.train(True)\n",
        "    model.train(is_train)\n",
        "\n",
        "    for batch in tqdm(batch_gen) :\n",
        "        start = time.time()\n",
        "        cnt_batches += 1\n",
        "\n",
        "        for k, v in batch.items():\n",
        "            batch[k] = v.to(device)\n",
        "\n",
        "        with torch.set_grad_enabled(is_train) :\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if is_train :\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            count += 1\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if cnt_batches % 100 == 0 and is_train: # изменила с 1000 до 100, так как с батчем в 16 получается 4800/16 = 300 батчей\n",
        "                # model.decoder.save_pretrained(f\"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_last_{model_type}_decoder\")\n",
        "                # model.encoder.save_pretrained(f\"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_last_{model_type}_encoder\")\n",
        "                model.save_pretrained(f\"/content/drive/MyDrive/ocr_models/tr_ocr_last_{model_type}\")\n",
        "\n",
        "                outputs = model.generate(val_batch[\"pixel_values\"].to(device))\n",
        "                now = compute_cer(pred_ids=outputs, label_ids=val_batch[\"labels\"])\n",
        "                s = 'val batch cer = ' + str(now) + ' '\n",
        "\n",
        "                outputs = model.generate(train_batch[\"pixel_values\"].to(device))\n",
        "                now = compute_cer(pred_ids=outputs, label_ids=train_batch[\"labels\"])\n",
        "                print(s + 'train batch cer = ', now)\n",
        "\n",
        "\n",
        "            if not is_train and full_cer:\n",
        "              outputs = model.generate(batch[\"pixel_values\"])\n",
        "              now = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
        "              cer += now\n",
        "\n",
        "    if full_cer:\n",
        "      now = cer / count\n",
        "    else:\n",
        "      outputs = model.generate(val_batch[\"pixel_values\"].to(device))\n",
        "      now = compute_cer(pred_ids=outputs, label_ids=val_batch[\"labels\"])\n",
        "\n",
        "    return epoch_loss / count, now\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtSxcQx7OoYg"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, verbose=True):\n",
        "    loader = {'train': train_loader, 'test': test_loader}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "    cer_history = {'train': [], 'test': []}\n",
        "    best_loss = 0.12595586647062199 # 0.6189\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if verbose:\n",
        "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "            print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'test']:\n",
        "            epoch_loss, epoch_cer = train_epoch(model, loader[phase], criterion, optimizer, phase == 'train', full_cer=True)\n",
        "            if verbose:\n",
        "                print('{} Loss: {:.4f} Cer: {:.4f}'.format(phase, epoch_loss, epoch_cer))\n",
        "            loss_history[phase].append(epoch_loss)\n",
        "            cer_history[phase].append(epoch_cer)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        if verbose:\n",
        "            show_random_predict(model, test_loader, val_batch)\n",
        "            print()\n",
        "\n",
        "        if loss_history['test'][-1] < best_loss:\n",
        "          best_loss = loss_history['test'][-1]\n",
        "          print('updated best loss on {} epoch, now it {}'.format(epoch, best_loss))\n",
        "          # model.decoder.save_pretrained(f\"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_best_{model_type}_decoder\")\n",
        "          # model.encoder.save_pretrained(f\"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_best_{model_type}_encoder\")\n",
        "          model.save_pretrained(f\"/content/drive/MyDrive/ocr_models/tr_ocr_best_{model_type}\")\n",
        "\n",
        "    return loss_history, cer_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJiRrddTx7VW"
      },
      "outputs": [],
      "source": [
        "epoch_loss, epoch_f1 = train_epoch(model, val_loader, None, None, False, True)\n",
        "epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JSkoXVW1-dR"
      },
      "outputs": [],
      "source": [
        "# epoch_loss, epoch_f1 = train_epoch(model, val_loader, None, None, False, True)\n",
        "# epoch_loss, epoch_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbfr_IHQPDUS"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
        "lr_scheduler = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q28aCV9KazW8"
      },
      "outputs": [],
      "source": [
        "loss_history, cer_history = train_model(model, train_loader, val_loader, lr_scheduler, optimizer, None, 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "qbdpBgqUIdd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztgUVLhp374K"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "loss_history, cer_history = train_model(model, train_loader, val_loader, None, optimizer, None, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)"
      ],
      "metadata": {
        "id": "KJfX6j5AFmX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_scheduler\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
        "                          num_warmup_steps=500,\n",
        "                          num_training_steps=len(train_loader) * 5) #5 = num_epochs\n",
        "\n",
        "loss_history, cer_history = train_model(model, train_loader, val_loader, None, optimizer, scheduler, 5)"
      ],
      "metadata": {
        "id": "YGObpi4GFtwF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}