{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSHQD/HWR/blob/main/Full_text_recognition_mmdetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdsMA-YMe9Tx",
        "outputId": "33d9e9c6-737e-4cf7-dd2d-c0f92709e081"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cpMW74p8fEO4",
        "outputId": "815a5df3-0a07-413e-960c-c455154c0183"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import albumentations as A\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch, torchvision\n",
        "import warnings\n",
        "import sys\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import collections\n",
        "from torchvision import datasets, models, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StQFlbhxfFoV",
        "outputId": "c5a6e829-4cc5-437e-eed4-f5e21ed4585b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jhEqUKKeT1"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip1 = '/content/drive/MyDrive/train_recognition.zip'\n",
        "extract_to1 = '/content/train_recognition'\n",
        "\n",
        "with zipfile.ZipFile(zip1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to1)\n",
        "\n",
        "zip2 = '/content/drive/MyDrive/train_segmentation.zip'\n",
        "extract_to2 = '/content/train_segmentation'\n",
        "with zipfile.ZipFile(zip2, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lani3iXNfH_O"
      },
      "outputs": [],
      "source": [
        "project_dir = '/content'\n",
        "data_dir = 'train_recognition'\n",
        "# train_images = 'images'\n",
        "train_images = '/content/train_recognition/train_recognition/images'\n",
        "image_size = 256\n",
        "first_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me2n5NZ0xcTk"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uEEa1IFjxBXZ",
        "outputId": "4d42ca2d-79f2-438f-d286-cfaa68e6b814"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "!pip install -e mmdetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2dCMTN_s6Uw",
        "outputId": "245a4f0c-3f56-46ea-89de-26af2148c2e9"
      },
      "outputs": [],
      "source": [
        "!git clone https://bitbucket.org/william_rusnack/minimumboundingbox.git\n",
        "sys.path.append('minimumboundingbox')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XuAqnmr7GW1f",
        "outputId": "84b3f11e-2fab-4ff0-e108-2f65a156d7bc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "# !pip install transformers==4.28.1\n",
        "!pip install -q datasets jiwer\n",
        "!pip install sentencepiece -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI9r23FQfIY9"
      },
      "source": [
        "# Building datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "lJ8NYJvDfMDv",
        "outputId": "4827500c-a9bf-4871-e808-c05e215d9d27"
      },
      "outputs": [],
      "source": [
        "annotation = pd.read_csv('/content/train_recognition/train_recognition/labels.csv')\n",
        "annotation.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Nzy-WAjig778",
        "outputId": "c0b26aaa-1a20-4410-c87b-be1190b2c6fb"
      },
      "outputs": [],
      "source": [
        "image = plt.imread(os.path.join(train_images, '0.png'))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9NiU2aZhXXN",
        "outputId": "7d8e9b3f-8cc9-4c31-b4d6-dcb6d14cc935"
      },
      "outputs": [],
      "source": [
        "len(os.listdir(train_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-kEodl3iXGT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/train_segmentation/train_segmentation/annotations.json') as f:\n",
        "    seg_annotations = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gpXxqWkbUbp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "val_annot = seg_annotations.copy()\n",
        "train_annot = seg_annotations.copy()\n",
        "\n",
        "train_annot['images'], val_annot['images']  = train_test_split(seg_annotations['images'], test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRtMNqEebXcn"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_dir, '/content/train_segmentation/train_segmentation/annotations_val.json'), 'w') as outfile:\n",
        "    json.dump(val_annot, outfile)\n",
        "\n",
        "\n",
        "with open(os.path.join(data_dir, '/content/train_segmentation/train_segmentationannotations_train.json'), 'w') as outfile:\n",
        "    json.dump(train_annot, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6yrt_sJpw2R"
      },
      "source": [
        "# Train mask rcnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xXXpWTOQuYc"
      },
      "source": [
        "## Train mask-rcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfhpT1a8QyZ5"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_instance_segmentation_model(num_classes = 2):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfH3QFh8Q5h-",
        "outputId": "370e342b-bd1f-491b-8e30-4666b610445d"
      },
      "outputs": [],
      "source": [
        "model = get_instance_segmentation_model()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(params, lr=3e-4)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=20,\n",
        "                                               gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "XNjW452tl5vz",
        "outputId": "500eac61-b69e-4f47-d9f8-5bc808875027"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -U openmim\n",
        "!mim install \"mmcv>=2.0.0rc4,<2.1.0\"\n",
        "!mim install \"mmdet>=3.0.0,<3.2.0\"\n",
        "\n",
        "# Проверка\n",
        "import torch, mmcv, mmdet\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"MMCV:\", mmcv.__version__)\n",
        "print(\"MMDetection:\", mmdet.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6DNBsRdKyOY"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y torch torchvision torchaudio triton transformers\n",
        "# !pip install torch==1.13.1 torchvision==0.15.2 torchaudio==0.13.1\n",
        "# !pip install transformers==4.41.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CXbDIq6LRmQ",
        "outputId": "cb67263b-44d3-4860-c9f3-0a33dcf014e0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4TATSKagoXM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3sBUpCxkNMT",
        "outputId": "43e751bd-ccff-4328-a70b-99f6de44fc45"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(\"/content/train_recognition/train_recognition\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "CPrYYnyhjnH3",
        "outputId": "58483d81-0a80-4092-972a-3b82867a3115"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import functional as TF\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class CocoDataset(torchvision.datasets.CocoDetection):\n",
        "    def __getitem__(self, idx):\n",
        "        image, annotations = super().__getitem__(idx)\n",
        "\n",
        "        boxes, labels, masks = [], [], []\n",
        "        for ann in annotations:\n",
        "            bbox = ann[\"bbox\"]\n",
        "            x, y, w, h = bbox\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(1)  # только один класс \"text\"\n",
        "            masks.append(self.coco.annToMask(ann))\n",
        "\n",
        "        image = TF.to_tensor(image)\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
        "            \"masks\": torch.tensor(masks, dtype=torch.uint8),\n",
        "            \"image_id\": torch.tensor([idx])\n",
        "        }\n",
        "\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# Параметры путей\n",
        "data_root = \"/content/train_recognition/train_recognition\"\n",
        "train_dataset = CocoDataset(\n",
        "    root=os.path.join(data_root, \"images\"),\n",
        "    annFile=os.path.join(data_root, \"labels.csv\")\n",
        ")\n",
        "\n",
        "# Разделим датасет (если нет отдельного val)\n",
        "train_size = int(0.9 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qdkwBWMp7e51"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, num_epochs, model_name, device='cuda'):\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs - 1}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # --- TRAINING ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        loss_log = {}\n",
        "\n",
        "        for images, targets in tqdm(train_loader, desc=\"Train\", leave=False):\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += losses.item()\n",
        "\n",
        "            for k, v in loss_dict.items():\n",
        "                loss_log[k] = loss_log.get(k, 0) + v.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        print(\"train Loss:\", {k: round(v / len(train_loader), 6) for k, v in loss_log.items()})\n",
        "\n",
        "        # --- VALIDATION ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_log = {}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in tqdm(val_loader, desc=\"Val\", leave=False):\n",
        "                images = [img.to(device) for img in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                loss_dict = model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                val_loss += losses.item()\n",
        "                for k, v in loss_dict.items():\n",
        "                    val_log[k] = val_log.get(k, 0) + v.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        print(\"test Loss:\", {k: round(v / len(val_loader), 6) for k, v in val_log.items()})\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            print(f\"updated best loss, now it {best_val_loss:.10f}\")\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "\n",
        "            # Визуализация результата на одной картинке\n",
        "            sample_img = images[0].cpu()\n",
        "            sample_target = targets[0]\n",
        "            sample_output = model([sample_img.to(device)])[0]\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.imshow(F.to_pil_image(sample_img))\n",
        "            plt.axis(\"off\")\n",
        "            plt.subplot(1, 2, 2)\n",
        "\n",
        "            mask = torch.zeros_like(sample_img[0])\n",
        "            for m in sample_output['masks']:\n",
        "                mask = torch.maximum(mask, (m[0] > 0.5).cpu())\n",
        "\n",
        "            plt.imshow(mask, cmap='rainbow', alpha=0.8)\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzoIyrIAqNzd"
      },
      "outputs": [],
      "source": [
        "\n",
        "history = train_model(model, train_loader, val_loader, None, optimizer, lr_scheduler, 30, 'mask_rcnn_small_2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7KZzbwhF9iY"
      },
      "source": [
        "# Generate poligons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsbjhwjjKnKf"
      },
      "source": [
        "### Get masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KSoyeRVxrys"
      },
      "outputs": [],
      "source": [
        "sys.path.append('mmdetection')\n",
        "\n",
        "import mmdet\n",
        "import mmcv\n",
        "# from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "\n",
        "from mmdet.datasets import build_dataset\n",
        "from mmdet.models import build_detector\n",
        "from mmdet.apis import train_detector\n",
        "# from mmcv import Config\n",
        "from mmdet.apis import init_detector, inference_detector, show_result_pyplot, set_random_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXgBnE3-qHLL"
      },
      "outputs": [],
      "source": [
        "class SegmentationModel:\n",
        "  def __init__(self, model_path, cfg):\n",
        "    with open('labels.txt', 'w') as f:\n",
        "      f.write('text')\n",
        "    self.cfg = cfg\n",
        "    self.cfg.load_from = model_path\n",
        "    self.model = init_detector(self.cfg, model_path, device=device)\n",
        "    self.model.CLASSES = ['text']\n",
        "    self.model.cfg = self.cfg\n",
        "    self.threshold = 0.3\n",
        "\n",
        "  def eval(self):\n",
        "    return self\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    result = mmdet.apis.inference.inference_detector(self.model, batch)\n",
        "    output = []\n",
        "    for img in result:\n",
        "      out_img = []\n",
        "      for box in img[0]:\n",
        "        out = []\n",
        "        #print(len(box))\n",
        "        left, top, right, bottom, c = box\n",
        "        if c > self.threshold:\n",
        "          out.append([[left, top]])\n",
        "          out.append([[right, top]])\n",
        "          out.append([[right, bottom]])\n",
        "          out.append([[left, bottom]])\n",
        "          out_img.append(out)\n",
        "      output.append(out_img)\n",
        "    return np.array(output).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL4gWjf-q8R-"
      },
      "outputs": [],
      "source": [
        "import config\n",
        "importlib.reload(config)\n",
        "\n",
        "seg_model = SegmentationModel('/content/drive/MyDrive/segmentation_models',\n",
        "                              config.cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hodQlW9urvxa"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('/content/train_segmentation/images/0_0.jpg')\n",
        "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "contours = seg_model([image])\n",
        "\n",
        "for contour in contours[0]:\n",
        "    cv2.drawContours(image, np.array([contour.astype(int)]), -1, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_9m-2Gbarhk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSnLpn0FGHEK"
      },
      "source": [
        "# Loading TrOcr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfu8ygF3IwQV"
      },
      "outputs": [],
      "source": [
        "def plot_images(images_for_show):\n",
        "  fig = plt.figure(figsize=(16, 16))\n",
        "\n",
        "  columns = len(images_for_show)\n",
        "  rows = 1\n",
        "  for i in range(1, columns*rows +1):\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.clip(images_for_show[i - 1], 0, 1))\n",
        "\n",
        "  fig.subplots_adjust(wspace=0.1, hspace=0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow-8bQqjHAGy"
      },
      "outputs": [],
      "source": [
        "class AlbuPadding(A.DualTransform):\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(AlbuPadding, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, image, **params):\n",
        "        zeros = np.zeros((128, 384, 3))\n",
        "        image = np.concatenate([zeros, image, zeros], axis=0)\n",
        "        return image.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW-GwkyyKsTT"
      },
      "outputs": [],
      "source": [
        "from albumentations.pytorch.transforms import ToTensor\n",
        "from transformers import AutoFeatureExtractor, XLMRobertaTokenizer, VisionEncoderDecoderModel, RobertaTokenizer\n",
        "\n",
        "class TrOcrModel:\n",
        "  def __init__(self, model_path, padding=True):\n",
        "      self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(device)\n",
        "      self.model.eval()\n",
        "\n",
        "      self.feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/trocr-small-handwritten')\n",
        "      self.tokenizer = XLMRobertaTokenizer.from_pretrained('microsoft/trocr-small-handwritten')\n",
        "\n",
        "      if padding:\n",
        "        self.transforms = A.Compose([\n",
        "                A.Resize(128, 384),\n",
        "                AlbuPadding(always_apply=True),\n",
        "            ])\n",
        "      else:\n",
        "        self.transforms = A.Compose([\n",
        "                A.Resize(384, 384),\n",
        "            ])\n",
        "\n",
        "  def image_preprocess(self, image):\n",
        "      image = self.transforms(image=image)['image']\n",
        "      pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
        "      return pixel_values\n",
        "\n",
        "  def predict_batch(self, images):\n",
        "      batch = torch.concat([self.image_preprocess(image) for image in images], axis=0).to(device)\n",
        "      outputs = self.model.generate(batch)\n",
        "      return [self.tokenizer.decode(pred.cpu().numpy(), skip_special_tokens=True) for pred in outputs]\n",
        "\n",
        "  def __call__(self, image):\n",
        "      pred = self.model.generate(self.image_preprocess(image).to(device))\n",
        "      return self.tokenizer.decode(pred[0].cpu().numpy(), skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDPStb_oI4Fe"
      },
      "outputs": [],
      "source": [
        "text_model = TrOcrModel(\"/content/drive/MyDrive/ocr_model_last\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cStLSONHISml"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(os.path.join(train_images, '0.png'))\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN5kxznvJ1bk"
      },
      "outputs": [],
      "source": [
        "text_model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR8wLpNPKKrQ"
      },
      "outputs": [],
      "source": [
        "text_model.predict_batch([image, image])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPHOgzO0JpVc"
      },
      "source": [
        "# Making predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2OR667LMdiP"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFont, ImageDraw, Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdAwD-8Dy-WG"
      },
      "outputs": [],
      "source": [
        "def crop_img_by_polygon(img, polygon):\n",
        "    pts = np.array(polygon)\n",
        "    rect = cv2.boundingRect(pts)\n",
        "    x,y,w,h = rect\n",
        "    croped = img[y:y+h, x:x+w]\n",
        "    pts = pts - pts.min(axis=0)\n",
        "    mask = np.zeros(croped.shape[:2], np.uint8)\n",
        "    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
        "    dst = cv2.bitwise_and(croped, croped, mask=mask)\n",
        "    return dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX-o7OD8Jqni"
      },
      "outputs": [],
      "source": [
        "def get_image_visualization(img, pred_data, fontpath, font_koef=50):\n",
        "    img = img.copy()\n",
        "    h, w = img.shape[:2]\n",
        "    font = ImageFont.truetype(fontpath, int(h/font_koef))\n",
        "    empty_img = Image.new('RGB', (w, h), (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(empty_img)\n",
        "\n",
        "    for prediction in pred_data['predictions']:\n",
        "        polygon = prediction['polygon']\n",
        "        pred_text = prediction['text']\n",
        "        cv2.drawContours(img, np.array([polygon]), -1, (0, 255, 0), 2)\n",
        "        x, y, w, h = cv2.boundingRect(np.array([polygon]))\n",
        "        draw.text((x, y), pred_text, fill=0, font=font)\n",
        "        # print(pred_text, x, y)\n",
        "\n",
        "    vis_img = np.array(empty_img)\n",
        "    vis = np.concatenate((img, vis_img), axis=1)\n",
        "    return vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1UvLO9uRfpr"
      },
      "outputs": [],
      "source": [
        "def get_polygon_for_answer(polygon, croped):\n",
        "    pts = np.array(polygon)\n",
        "    rect = cv2.boundingRect(pts)\n",
        "    x1,y1,w,h = rect\n",
        "    mid_x = x1 + w // 2\n",
        "    mid_y = y1 + h // 2\n",
        "\n",
        "    best = 1e9\n",
        "    for i in range(h):\n",
        "      now = abs((croped[:i, :] != [0, 0, 0]).sum() - (croped[i:, :] != [0, 0, 0]).sum())\n",
        "      if now < best:\n",
        "        best = now\n",
        "        mid_y = i + y1\n",
        "\n",
        "    x1 = mid_x - w // 5\n",
        "    x2 = mid_x + w // 5\n",
        "    return [(x1, mid_y), (x1, mid_y + 1), (x2, mid_y + 1), (x2, mid_y)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUqF5p3Jas1g"
      },
      "outputs": [],
      "source": [
        "def get_classifier_model():\n",
        "\n",
        "  model = models.resnet50(pretrained=True)\n",
        "  model.fc = nn.Sequential(\n",
        "      nn.Linear(2048, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(256, 2)\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9liNBdgsem5J"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_dir, '/content/train_segmentation_small/train_segmentation_small/annotations.json'), 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "def get_image_cer(image_name):\n",
        "  img = cv2.imread('/content/train_segmentation_small/train_segmentation_small/images/' + image_name)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  pred_data = {}\n",
        "  pred_data[image_name] = pipeline_model(img)\n",
        "  with open('prediction.json', \"w\") as f:\n",
        "    json.dump(pred_data, f)\n",
        "\n",
        "  now = annotations.copy()\n",
        "  for el in annotations['images']:\n",
        "    if el['file_name'] == image_name:\n",
        "      now['images'] = [el]\n",
        "      break\n",
        "\n",
        "\n",
        "  with open(os.path.join('annotations_now.json'), 'w') as outfile:\n",
        "      json.dump(now, outfile)\n",
        "  print('predict is ready')\n",
        "\n",
        "  print(os.popen('python3 data/evaluate.py --ref_path annotations_now.json --pred_path prediction.json').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnVUV7rttcqj"
      },
      "outputs": [],
      "source": [
        "np.argsort([0, 3, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLWIQ1ECKhic"
      },
      "outputs": [],
      "source": [
        "import config\n",
        "\n",
        "class PiepleinePredictor:\n",
        "    def __init__(self, segm_model_path, ru_ocr_model_path, en_ocr_model_path, classifier_model_path):\n",
        "        # self.seg_model = SEGMpredictor(segm_model_path)\n",
        "        self.seg_model = SegmentationModel(segm_model_path, config.cfg)\n",
        "\n",
        "        self.text_model = {'ru': TrOcrModel(ru_ocr_model_path),\n",
        "                           'en': TrOcrModel(en_ocr_model_path)}\n",
        "        self.batch_size = 8\n",
        "        self.transforms = A.RandomScale(scale_limit=(-0.5, -0.5), p=1)\n",
        "\n",
        "        self.classifier_model = torch.load(classifier_model_path, map_location=device)\n",
        "        self.classifier_predicts = 25\n",
        "        self.classifier_transforms = A.Compose([\n",
        "                  A.Resize(128, 384),\n",
        "                  ToTensor()\n",
        "              ])\n",
        "\n",
        "    def predict_language(self, images):\n",
        "        p = np.argsort([el.shape[1] for el in images])\n",
        "        batch = []\n",
        "        for idx in p[-self.classifier_predicts:]:\n",
        "          batch.append(self.classifier_transforms(image=images[idx])['image'])\n",
        "\n",
        "        batch = torch.stack(batch).to(device)\n",
        "        preds = self.classifier_model(batch)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        # plot_images([torch.moveaxis(el, 0, -1).detach().cpu().numpy() for el in batch[:10]])\n",
        "        # print(preds[:10])\n",
        "        res = preds.sum() > len(preds) / 2\n",
        "        return 'ru' if res else 'en'\n",
        "\n",
        "    def __call__(self, img, return_only_language=False):\n",
        "        img = img.copy()\n",
        "        # img = self.transforms(image=img)['image']\n",
        "        with torch.no_grad():\n",
        "          output = {'predictions': []}\n",
        "          bgr = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2BGR)\n",
        "          contours = self.seg_model([bgr])[0]\n",
        "          images = []\n",
        "          not_none_contours = []\n",
        "          for contour in contours:\n",
        "              if contour is not None:\n",
        "                  crop = crop_img_by_polygon(img, contour)\n",
        "                  images.append(crop)\n",
        "                  not_none_contours.append(contour)\n",
        "\n",
        "          language = self.predict_language(images)\n",
        "          if return_only_language:\n",
        "            return language\n",
        "\n",
        "          predicted_text = []\n",
        "          for i in range(0, len(images), self.batch_size):\n",
        "            predicted_text += self.text_model[language].predict_batch(images[i:i + self.batch_size])\n",
        "\n",
        "\n",
        "          for pred_text, contour, crop in zip(predicted_text, not_none_contours, images):\n",
        "            output['predictions'].append({\n",
        "                              # 'polygon': [[int(i[0][0] * 2), int(i[0][1] * 2)] for i in contour],\n",
        "                              # 'polygon': [[int(i[0][0]), int(i[0][1])] for i in contour],\n",
        "                              'polygon': get_polygon_for_answer(contour, crop),\n",
        "                              'text': pred_text\n",
        "                            })\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOBVR8QPKkm0"
      },
      "outputs": [],
      "source": [
        "pipeline_model = PiepleinePredictor('/content/drive/MyDrive/mask_rcnn_small/segm_model_final.pth',\n",
        "                                    \"/content/drive/MyDrive/tr_ocr_best_small\",\n",
        "                                    \"/content/drive/MyDrive/tr_ocr_best\",\n",
        "                                    \"/content/drive/MyDrive/weights_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3igMC1PlDyR"
      },
      "outputs": [],
      "source": [
        "get_image_cer('0_0_eng.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdMTfW6yxWCv"
      },
      "outputs": [],
      "source": [
        "get_image_cer('100_0.JPG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRSVLjlfPJVE"
      },
      "outputs": [],
      "source": [
        "get_image_cer('6_1_eng.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH5K1VMRp0nI"
      },
      "outputs": [],
      "source": [
        "path = '/content/train_segmentation_small/train_segmentation_small/images'\n",
        "\n",
        "sum = 0\n",
        "right = 0\n",
        "for image_name in tqdm(os.listdir(path)[:]):\n",
        "  real = 'en' if 'eng' in image_name else 'ru'\n",
        "  img = cv2.imread(os.path.join(path, image_name))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  pred = pipeline_model(img, True)\n",
        "  sum += 1\n",
        "  right += pred == real\n",
        "  if pred != real:\n",
        "    print(image_name, real)\n",
        "\n",
        "print()\n",
        "print(right, sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBlBRha9M0A2"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('/content/train_segmentation_small/train_segmentation_small/images/10_0.JPG')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG-kVpeNBGX1"
      },
      "outputs": [],
      "source": [
        "output = pipeline_model(img)\n",
        "vis = get_image_visualization(img, output, os.path.join(data_dir, 'font.otf'))\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(vis)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz9kNbVqQjyk"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('/content/train_segmentation_small/train_segmentation_small/images/105_0.JPG')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img = A.RandomScale(scale_limit=(-0.5, -0.5), p=1)(image=img)['image']\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DofAmxhlqQtl"
      },
      "source": [
        "# Save predictions from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6-2K0wqi-l"
      },
      "outputs": [],
      "source": [
        "DATA_JSON_PATH = '/content/train_segmentation_small/train_segmentation_small/annotations_val.json'\n",
        "IMAGE_ROOT = '/content/train_segmentation_small/train_segmentation_small/images/'\n",
        "SAVE_PATH = os.path.join(project_dir, 'prediction.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4PS1b6mqsAl"
      },
      "outputs": [],
      "source": [
        "with open(DATA_JSON_PATH, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "pred_data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxtx0enbBEOo"
      },
      "outputs": [],
      "source": [
        "for data_img in tqdm(data['images']):\n",
        "    img_name = data_img['file_name']\n",
        "    image = cv2.imread(os.path.join(IMAGE_ROOT, img_name))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    pred_data[img_name] = pipeline_model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uZZHo59qtPe"
      },
      "outputs": [],
      "source": [
        "for data_img in tqdm(data['images']):\n",
        "    img_name = data_img['file_name']\n",
        "    image = cv2.imread(os.path.join(IMAGE_ROOT, img_name))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    pred_data[img_name] = pipeline_model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XNzKwQRthzO"
      },
      "outputs": [],
      "source": [
        "with open('prediction.json', \"w\") as f:\n",
        "    json.dump(pred_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws83q32EKB4N"
      },
      "outputs": [],
      "source": [
        "#small\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uWBkxzAakf8"
      },
      "outputs": [],
      "source": [
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c52GOBA3llZ"
      },
      "outputs": [],
      "source": [
        "# mask_rcnn_small + en_ocr_padding + classifier not regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moO47JA3q6dB"
      },
      "outputs": [],
      "source": [
        "# mask_rcnn_small + en_ocr_padding + classifier regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQIOve6NTzIS"
      },
      "outputs": [],
      "source": [
        "# mask_rcnn_small + en_ocr_padding + classifier not regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14Qz6F_dE1Np"
      },
      "outputs": [],
      "source": [
        "# detectoRS + en_ocr + classifier not regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TVmM-nxzi7l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kSnLpn0FGHEK"
      ],
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
