{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSHQD/HWR/blob/main/Full_text_recognition_mmdetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-mUDQ892RLFq"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# print(\"version:\", sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LdsMA-YMe9Tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4d39f2-9735-4b4a-9d18-25b5db0a922d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpMW74p8fEO4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import albumentations as A\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch, torchvision\n",
        "import warnings\n",
        "import sys\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import collections\n",
        "from torchvision import datasets, models, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StQFlbhxfFoV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5jhEqUKKeT1"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip1 = '/content/drive/MyDrive/train_recognition.zip'\n",
        "extract_to1 = '/content/train_recognition'\n",
        "\n",
        "with zipfile.ZipFile(zip1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to1)\n",
        "\n",
        "zip2 = '/content/drive/MyDrive/train_segmentation.zip'\n",
        "extract_to2 = '/content/train_segmentation'\n",
        "with zipfile.ZipFile(zip2, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lani3iXNfH_O"
      },
      "outputs": [],
      "source": [
        "# data_dir = '/content/drive/MyDrive/НТИ ИИ /data_team/'\n",
        "# project_dir = '/content/drive/MyDrive/НТИ ИИ /'\n",
        "# data_dir = 'data'\n",
        "project_dir = '/content'\n",
        "data_dir = 'train_recognition'\n",
        "train_images = 'images'\n",
        "train_images = '/content/train_recognition/train_recognition/images'\n",
        "image_size = 256\n",
        "first_size = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me2n5NZ0xcTk"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uEEa1IFjxBXZ"
      },
      "outputs": [],
      "source": [
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10/index.html\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "!cd mmdetection && pip install -e . && python setup.py develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yiDouKpf4CF"
      },
      "outputs": [],
      "source": [
        "# !unzip -q '/content/drive/MyDrive/train_segmentation.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2dCMTN_s6Uw"
      },
      "outputs": [],
      "source": [
        "# !git clone https://bitbucket.org/william_rusnack/minimumboundingbox.git\n",
        "# sys.path.append('minimumboundingbox')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuAqnmr7GW1f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install -q datasets jiwer\n",
        "!pip install sentencepiece -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI9r23FQfIY9"
      },
      "source": [
        "# Building datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ8NYJvDfMDv"
      },
      "outputs": [],
      "source": [
        "# labels_path = os.path.join(project_dir, data_dir, 'labels.csv')\n",
        "annotation = pd.read_csv('/content/train_recognition/train_recognition/labels.csv')\n",
        "# annotation = pd.read_csv(labels_path)\n",
        "annotation.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzy-WAjig778"
      },
      "outputs": [],
      "source": [
        "image = plt.imread(os.path.join(train_images, '0.png'))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9NiU2aZhXXN"
      },
      "outputs": [],
      "source": [
        "len(os.listdir(train_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-kEodl3iXGT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/train_segmentation/train_segmentation/annotations.json') as f:\n",
        "    seg_annotations = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gpXxqWkbUbp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "val_annot = seg_annotations.copy()\n",
        "train_annot = seg_annotations.copy()\n",
        "\n",
        "train_annot['images'], val_annot['images']  = train_test_split(seg_annotations['images'], test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRtMNqEebXcn"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_dir, '/content/train_segmentation/train_segmentation/annotations_val.json'), 'w') as outfile:\n",
        "    json.dump(val_annot, outfile)\n",
        "\n",
        "\n",
        "with open(os.path.join(data_dir, '/content/train_segmentation/train_segmentationannotations_train.json'), 'w') as outfile:\n",
        "    json.dump(train_annot, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6yrt_sJpw2R"
      },
      "source": [
        "# Train mask rcnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xXXpWTOQuYc"
      },
      "source": [
        "## Train mask-rcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfhpT1a8QyZ5"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_instance_segmentation_model(num_classes = 2):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfH3QFh8Q5h-"
      },
      "outputs": [],
      "source": [
        "model = get_instance_segmentation_model()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(params, lr=3e-4)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=20,\n",
        "                                               gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XNjW452tl5vz"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -U openmim\n",
        "!mim install \"mmcv>=2.0.0rc4,<2.1.0\"\n",
        "!mim install \"mmdet>=3.0.0,<3.2.0\"\n",
        "\n",
        "# Проверка\n",
        "import torch, mmcv, mmdet\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"MMCV:\", mmcv.__version__)\n",
        "print(\"MMDetection:\", mmdet.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmigNx2OlaPK"
      },
      "outputs": [],
      "source": [
        "from mmdet.datasets import build_dataset\n",
        "from mmengine.dataset import build_dataloader\n",
        "\n",
        "# Загрузка конфигурации\n",
        "from mmengine.config import Config\n",
        "cfg = Config.fromfile('configs/mask_rcnn/mask_rcnn_x101_32x8d_fpn_mstrain-poly_3x_coco.py')\n",
        "\n",
        "# Настройка путей и классов\n",
        "cfg.dataset_type = 'CocoDataset'\n",
        "cfg.data_root = '/content/train_recognition/'\n",
        "cfg.data.train.ann_file = 'labels.json'\n",
        "cfg.data.train.img_prefix = 'images'\n",
        "cfg.data.train.classes = ('text',)\n",
        "\n",
        "# Создание датасета и загрузчика\n",
        "datasets = [build_dataset(cfg.data.train)]\n",
        "train_loader = build_dataloader(\n",
        "    datasets[0],\n",
        "    samples_per_gpu=2,\n",
        "    workers_per_gpu=2,\n",
        "    dist=False,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5kLL9LuinN0"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, lr_scheduler, num_epochs, model_name):\n",
        "    model.to(device)\n",
        "    history = {'train_loss': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for images, targets in train_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += losses.item()\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_loss)\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Сохраняем модель\n",
        "    torch.save(model.state_dict(), f'{model_name}.pth')\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzoIyrIAqNzd"
      },
      "outputs": [],
      "source": [
        "history = train_model(model, train_loader, val_loader, None, optimizer, lr_scheduler, 30, 'mask_rcnn_small_2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7KZzbwhF9iY"
      },
      "source": [
        "# Generate poligons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsbjhwjjKnKf"
      },
      "source": [
        "### Get masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KSoyeRVxrys"
      },
      "outputs": [],
      "source": [
        "sys.path.append('mmdetection')\n",
        "\n",
        "import mmdet\n",
        "import mmcv\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "\n",
        "from mmdet.datasets import build_dataset\n",
        "from mmdet.models import build_detector\n",
        "from mmdet.apis import train_detector\n",
        "from mmcv import Config\n",
        "from mmdet.apis import init_detector, inference_detector, show_result_pyplot, set_random_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXgBnE3-qHLL"
      },
      "outputs": [],
      "source": [
        "class SegmentationModel:\n",
        "  def __init__(self, model_path, cfg):\n",
        "    with open('labels.txt', 'w') as f:\n",
        "      f.write('text')\n",
        "    self.cfg = cfg\n",
        "    self.cfg.load_from = model_path\n",
        "    self.model = init_detector(self.cfg, model_path, device=device)\n",
        "    self.model.CLASSES = ['text']\n",
        "    self.model.cfg = self.cfg\n",
        "    self.threshold = 0.3\n",
        "\n",
        "  def eval(self):\n",
        "    return self\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    result = mmdet.apis.inference.inference_detector(self.model, batch)\n",
        "    output = []\n",
        "    for img in result:\n",
        "      out_img = []\n",
        "      for box in img[0]:\n",
        "        out = []\n",
        "        #print(len(box))\n",
        "        left, top, right, bottom, c = box\n",
        "        if c > self.threshold:\n",
        "          out.append([[left, top]])\n",
        "          out.append([[right, top]])\n",
        "          out.append([[right, bottom]])\n",
        "          out.append([[left, bottom]])\n",
        "          out_img.append(out)\n",
        "      output.append(out_img)\n",
        "    return np.array(output).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL4gWjf-q8R-"
      },
      "outputs": [],
      "source": [
        "import config\n",
        "importlib.reload(config)\n",
        "\n",
        "seg_model = SegmentationModel('/content/drive/MyDrive/НТИ ИИ /team/semeka_models/epoch_21.pth',\n",
        "                              config.cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hodQlW9urvxa"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread('/content/train_segmentation/images/0_0_eng.jpg')\n",
        "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "contours = seg_model([image])\n",
        "\n",
        "for contour in contours[0]:\n",
        "    cv2.drawContours(image, np.array([contour.astype(int)]), -1, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_9m-2Gbarhk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSnLpn0FGHEK"
      },
      "source": [
        "# Loading TrOcr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfu8ygF3IwQV"
      },
      "outputs": [],
      "source": [
        "def plot_images(images_for_show):\n",
        "  \"\"\"Строит изображение на одном графике\"\"\"\n",
        "  fig = plt.figure(figsize=(16, 16))\n",
        "\n",
        "  columns = len(images_for_show)\n",
        "  rows = 1\n",
        "  for i in range(1, columns*rows +1):\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(np.clip(images_for_show[i - 1], 0, 1))\n",
        "\n",
        "  fig.subplots_adjust(wspace=0.1, hspace=0)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow-8bQqjHAGy"
      },
      "outputs": [],
      "source": [
        "class AlbuPadding(A.DualTransform):\n",
        "    def __init__(self, always_apply=False, p=0.5):\n",
        "        super(AlbuPadding, self).__init__(always_apply, p)\n",
        "\n",
        "    def apply(self, image, **params):\n",
        "        zeros = np.zeros((128, 384, 3))\n",
        "        image = np.concatenate([zeros, image, zeros], axis=0)\n",
        "        return image.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW-GwkyyKsTT"
      },
      "outputs": [],
      "source": [
        "from albumentations.pytorch.transforms import ToTensor\n",
        "from transformers import AutoFeatureExtractor, XLMRobertaTokenizer, VisionEncoderDecoderModel, RobertaTokenizer\n",
        "\n",
        "class TrOcrModel:\n",
        "  def __init__(self, model_path, padding=True):\n",
        "      self.model = VisionEncoderDecoderModel.from_pretrained(model_path).to(device)\n",
        "      self.model.eval()\n",
        "\n",
        "      self.feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/trocr-small-handwritten')\n",
        "      self.tokenizer = XLMRobertaTokenizer.from_pretrained('microsoft/trocr-small-handwritten')\n",
        "\n",
        "      if padding:\n",
        "        self.transforms = A.Compose([\n",
        "                A.Resize(128, 384),\n",
        "                AlbuPadding(always_apply=True),\n",
        "            ])\n",
        "      else:\n",
        "        self.transforms = A.Compose([\n",
        "                A.Resize(384, 384),\n",
        "            ])\n",
        "\n",
        "  def image_preprocess(self, image):\n",
        "      image = self.transforms(image=image)['image']\n",
        "      pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
        "      return pixel_values\n",
        "\n",
        "  def predict_batch(self, images):\n",
        "      batch = torch.concat([self.image_preprocess(image) for image in images], axis=0).to(device)\n",
        "      outputs = self.model.generate(batch)\n",
        "      return [self.tokenizer.decode(pred.cpu().numpy(), skip_special_tokens=True) for pred in outputs]\n",
        "\n",
        "  def __call__(self, image):\n",
        "      pred = self.model.generate(self.image_preprocess(image).to(device))\n",
        "      return self.tokenizer.decode(pred[0].cpu().numpy(), skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDPStb_oI4Fe"
      },
      "outputs": [],
      "source": [
        "text_model = TrOcrModel(\"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_best_small_aug_nti2data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cStLSONHISml"
      },
      "outputs": [],
      "source": [
        "image = cv2.imread(os.path.join(train_images, '0.png'))\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN5kxznvJ1bk"
      },
      "outputs": [],
      "source": [
        "text_model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR8wLpNPKKrQ"
      },
      "outputs": [],
      "source": [
        "text_model.predict_batch([image, image])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPHOgzO0JpVc"
      },
      "source": [
        "# Making predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2OR667LMdiP"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFont, ImageDraw, Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdAwD-8Dy-WG"
      },
      "outputs": [],
      "source": [
        "def crop_img_by_polygon(img, polygon):\n",
        "    # https://stackoverflow.com/questions/48301186/cropping-concave-polygon-from-image-using-opencv-python\n",
        "    pts = np.array(polygon)\n",
        "    rect = cv2.boundingRect(pts)\n",
        "    x,y,w,h = rect\n",
        "    croped = img[y:y+h, x:x+w]\n",
        "    pts = pts - pts.min(axis=0)\n",
        "    mask = np.zeros(croped.shape[:2], np.uint8)\n",
        "    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n",
        "    dst = cv2.bitwise_and(croped, croped, mask=mask)\n",
        "    return dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX-o7OD8Jqni"
      },
      "outputs": [],
      "source": [
        "def get_image_visualization(img, pred_data, fontpath, font_koef=50):\n",
        "    img = img.copy()\n",
        "    h, w = img.shape[:2]\n",
        "    font = ImageFont.truetype(fontpath, int(h/font_koef))\n",
        "    empty_img = Image.new('RGB', (w, h), (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(empty_img)\n",
        "\n",
        "    for prediction in pred_data['predictions']:\n",
        "        polygon = prediction['polygon']\n",
        "        pred_text = prediction['text']\n",
        "        cv2.drawContours(img, np.array([polygon]), -1, (0, 255, 0), 2)\n",
        "        x, y, w, h = cv2.boundingRect(np.array([polygon]))\n",
        "        draw.text((x, y), pred_text, fill=0, font=font)\n",
        "        # print(pred_text, x, y)\n",
        "\n",
        "    vis_img = np.array(empty_img)\n",
        "    vis = np.concatenate((img, vis_img), axis=1)\n",
        "    return vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1UvLO9uRfpr"
      },
      "outputs": [],
      "source": [
        "def get_polygon_for_answer(polygon, croped):\n",
        "    pts = np.array(polygon)\n",
        "    rect = cv2.boundingRect(pts)\n",
        "    x1,y1,w,h = rect\n",
        "    mid_x = x1 + w // 2\n",
        "    mid_y = y1 + h // 2\n",
        "\n",
        "    best = 1e9\n",
        "    for i in range(h):\n",
        "      now = abs((croped[:i, :] != [0, 0, 0]).sum() - (croped[i:, :] != [0, 0, 0]).sum())\n",
        "      if now < best:\n",
        "        best = now\n",
        "        mid_y = i + y1\n",
        "\n",
        "    x1 = mid_x - w // 5\n",
        "    x2 = mid_x + w // 5\n",
        "    # return [(mid_y, x1), (mid_y + 5, x1), (mid_y + 5, x2), (mid_y, x2)]\n",
        "    return [(x1, mid_y), (x1, mid_y + 1), (x2, mid_y + 1), (x2, mid_y)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUqF5p3Jas1g"
      },
      "outputs": [],
      "source": [
        "def get_classifier_model():\n",
        "\n",
        "  model = models.resnet50(pretrained=True)\n",
        "  model.fc = nn.Sequential(\n",
        "      nn.Linear(2048, 256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(256, 2)\n",
        "  )\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9liNBdgsem5J"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_dir, 'train_segmentation/annotations.json'), 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "def get_image_cer(image_name):\n",
        "  img = cv2.imread('data/train_segmentation/images/' + image_name)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  pred_data = {}\n",
        "  pred_data[image_name] = pipeline_model(img)\n",
        "  with open('prediction.json', \"w\") as f:\n",
        "    json.dump(pred_data, f)\n",
        "\n",
        "  now = annotations.copy()\n",
        "  for el in annotations['images']:\n",
        "    if el['file_name'] == image_name:\n",
        "      now['images'] = [el]\n",
        "      break\n",
        "\n",
        "\n",
        "  with open(os.path.join('annotations_now.json'), 'w') as outfile:\n",
        "      json.dump(now, outfile)\n",
        "  print('predict is ready')\n",
        "\n",
        "  print(os.popen('python3 data/evaluate.py --ref_path annotations_now.json --pred_path prediction.json').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnVUV7rttcqj"
      },
      "outputs": [],
      "source": [
        "np.argsort([0, 3, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLWIQ1ECKhic"
      },
      "outputs": [],
      "source": [
        "import config\n",
        "\n",
        "class PiepleinePredictor:\n",
        "    def __init__(self, segm_model_path, ru_ocr_model_path, en_ocr_model_path, classifier_model_path):\n",
        "        # self.seg_model = SEGMpredictor(segm_model_path)\n",
        "        self.seg_model = SegmentationModel(segm_model_path, config.cfg)\n",
        "\n",
        "        self.text_model = {'ru': TrOcrModel(ru_ocr_model_path),\n",
        "                           'en': TrOcrModel(en_ocr_model_path)}\n",
        "        self.batch_size = 8\n",
        "        self.transforms = A.RandomScale(scale_limit=(-0.5, -0.5), p=1)\n",
        "\n",
        "        self.classifier_model = torch.load(classifier_model_path, map_location=device)\n",
        "        self.classifier_predicts = 25\n",
        "        self.classifier_transforms = A.Compose([\n",
        "                  A.Resize(128, 384),\n",
        "                  ToTensor()\n",
        "              ])\n",
        "\n",
        "    def predict_language(self, images):\n",
        "        p = np.argsort([el.shape[1] for el in images])\n",
        "        batch = []\n",
        "        for idx in p[-self.classifier_predicts:]:\n",
        "          batch.append(self.classifier_transforms(image=images[idx])['image'])\n",
        "\n",
        "        batch = torch.stack(batch).to(device)\n",
        "        preds = self.classifier_model(batch)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        # plot_images([torch.moveaxis(el, 0, -1).detach().cpu().numpy() for el in batch[:10]])\n",
        "        # print(preds[:10])\n",
        "        res = preds.sum() > len(preds) / 2\n",
        "        return 'ru' if res else 'en'\n",
        "\n",
        "    def __call__(self, img, return_only_language=False):\n",
        "        img = img.copy()\n",
        "        # img = self.transforms(image=img)['image']\n",
        "        with torch.no_grad():\n",
        "          output = {'predictions': []}\n",
        "          bgr = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2BGR)\n",
        "          contours = self.seg_model([bgr])[0]\n",
        "          images = []\n",
        "          not_none_contours = []\n",
        "          for contour in contours:\n",
        "              if contour is not None:\n",
        "                  crop = crop_img_by_polygon(img, contour)\n",
        "                  images.append(crop)\n",
        "                  not_none_contours.append(contour)\n",
        "\n",
        "          language = self.predict_language(images)\n",
        "          if return_only_language:\n",
        "            return language\n",
        "\n",
        "          predicted_text = []\n",
        "          for i in range(0, len(images), self.batch_size):\n",
        "            predicted_text += self.text_model[language].predict_batch(images[i:i + self.batch_size])\n",
        "\n",
        "\n",
        "          # for img, text in zip(images[:30], predicted_text[:30]):\n",
        "          #   plt.imshow(img)\n",
        "          #   plt.title(text)\n",
        "          #   plt.show()\n",
        "\n",
        "          for pred_text, contour, crop in zip(predicted_text, not_none_contours, images):\n",
        "            output['predictions'].append({\n",
        "                              # 'polygon': [[int(i[0][0] * 2), int(i[0][1] * 2)] for i in contour],\n",
        "                              # 'polygon': [[int(i[0][0]), int(i[0][1])] for i in contour],\n",
        "                              'polygon': get_polygon_for_answer(contour, crop),\n",
        "                              'text': pred_text\n",
        "                            })\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOBVR8QPKkm0"
      },
      "outputs": [],
      "source": [
        "pipeline_model = PiepleinePredictor(\n",
        "                                    '/content/drive/MyDrive/НТИ ИИ /team/semeka_models/epoch_21.pth',\n",
        "                                    # '/content/drive/MyDrive/НТИ ИИ /team/sergey_models/mask_rcnn_small',\n",
        "                                    \"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_best_small\",\n",
        "                                    \"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/tr_ocr_best_eng_padding\",\n",
        "                                    \"/content/drive/MyDrive/НТИ ИИ /team/sergey_models/language_classifier\"\n",
        "                                    ) #_aug_nti2data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3igMC1PlDyR"
      },
      "outputs": [],
      "source": [
        "get_image_cer('0_0_eng.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdMTfW6yxWCv"
      },
      "outputs": [],
      "source": [
        "get_image_cer('100_0.JPG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRSVLjlfPJVE"
      },
      "outputs": [],
      "source": [
        "get_image_cer('6_1_eng.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH5K1VMRp0nI"
      },
      "outputs": [],
      "source": [
        "path = 'data/train_segmentation/images'\n",
        "\n",
        "sum = 0\n",
        "right = 0\n",
        "for image_name in tqdm(os.listdir(path)[:]):\n",
        "  real = 'en' if 'eng' in image_name else 'ru'\n",
        "  img = cv2.imread(os.path.join(path, image_name))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  pred = pipeline_model(img, True)\n",
        "  sum += 1\n",
        "  right += pred == real\n",
        "  if pred != real:\n",
        "    print(image_name, real)\n",
        "\n",
        "print()\n",
        "print(right, sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBlBRha9M0A2"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('data/train_segmentation/images/105_0.JPG')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG-kVpeNBGX1"
      },
      "outputs": [],
      "source": [
        "output = pipeline_model(img)\n",
        "vis = get_image_visualization(img, output, os.path.join(data_dir, 'font.otf'))\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(vis)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz9kNbVqQjyk"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('data/train_segmentation/images/105_0.JPG')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img = A.RandomScale(scale_limit=(-0.5, -0.5), p=1)(image=img)['image']\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DofAmxhlqQtl"
      },
      "source": [
        "# Save predictions from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6-2K0wqi-l"
      },
      "outputs": [],
      "source": [
        "# Путь к json'у с разметкой\n",
        "DATA_JSON_PATH = 'data/train_segmentation/annotations_val.json'\n",
        "\n",
        "# Папка с картинками тетрадей\n",
        "IMAGE_ROOT = 'data/train_segmentation/images/'\n",
        "\n",
        "# Файл для сохранения предсказаний пайплайна. Один json\n",
        "SAVE_PATH = os.path.join(project_dir, 'prediction.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4PS1b6mqsAl"
      },
      "outputs": [],
      "source": [
        "with open(DATA_JSON_PATH, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "pred_data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxtx0enbBEOo"
      },
      "outputs": [],
      "source": [
        "for data_img in tqdm(data['images']):\n",
        "    img_name = data_img['file_name']\n",
        "    image = cv2.imread(os.path.join(IMAGE_ROOT, img_name))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    pred_data[img_name] = pipeline_model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uZZHo59qtPe"
      },
      "outputs": [],
      "source": [
        "for data_img in tqdm(data['images']):\n",
        "    img_name = data_img['file_name']\n",
        "    image = cv2.imread(os.path.join(IMAGE_ROOT, img_name))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    pred_data[img_name] = pipeline_model(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XNzKwQRthzO"
      },
      "outputs": [],
      "source": [
        "with open('prediction.json', \"w\") as f:\n",
        "    json.dump(pred_data, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws83q32EKB4N"
      },
      "outputs": [],
      "source": [
        "#small\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uWBkxzAakf8"
      },
      "outputs": [],
      "source": [
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c52GOBA3llZ"
      },
      "outputs": [],
      "source": [
        "# mask_rcnn_small + en_ocr_padding + classifier not regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moO47JA3q6dB"
      },
      "outputs": [],
      "source": [
        "# mask_rcnn_small + en_ocr_padding + classifier regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQIOve6NTzIS"
      },
      "outputs": [],
      "source": [
        "# mask_rcnn_small + en_ocr_padding + classifier not regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14Qz6F_dE1Np"
      },
      "outputs": [],
      "source": [
        "# detectoRS + en_ocr + classifier not regression\n",
        "!python3 data/evaluate.py --ref_path data/train_segmentation/annotations_val.json --pred_path prediction.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TVmM-nxzi7l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "S7KZzbwhF9iY",
        "kSnLpn0FGHEK"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}